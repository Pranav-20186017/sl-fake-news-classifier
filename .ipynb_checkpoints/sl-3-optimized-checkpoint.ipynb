{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import split\n",
    "from os import path\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pylab as py\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "import csv, random, numpy, score, os, re, nltk, scipy, gensim\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from langdetect import detect\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    }
   ],
   "source": [
    "dataset = DataSet()\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bodies of training data points\n",
    "def get_bodies(data):\n",
    "    bodies = []\n",
    "    for i in range(len(data)):\n",
    "        bodies.append(dataset.articles[data[i]['Body ID']])\t\n",
    "    return bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the headlines of training data points\n",
    "def get_headlines(data):\n",
    "    headlines = []\n",
    "    for i in range(len(data)):\n",
    "        headlines.append(data[i]['Headline'])\n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation, Normalisation, Capitalisation, Non-alphanumeric removal, Stemming-Lemmatization\n",
    "def preprocess(string):\n",
    "    step1 = \" \".join(re.findall(r'\\w+', string, flags=re.UNICODE)).lower()\n",
    "    step2 = [lemmatizer.lemmatize(t).lower() for t in nltk.word_tokenize(step1)]\n",
    "    return step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting word overlap\n",
    "def extract_word_overlap(headlines, bodies):\n",
    "\tword_overlap = []\n",
    "\tfor i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "\t\tpreprocess_headline = preprocess(headline)\n",
    "\t\tpreprocess_body = preprocess(body)\n",
    "\t\tfeatures = len(set(preprocess_headline).intersection(preprocess_body)) / float(len(set(preprocess_headline).union(preprocess_body)))\n",
    "\t\tword_overlap.append(features)\n",
    "\n",
    "\t\t# Convert the list to a sparse matrix (in order to concatenate the cos sim with other features)\n",
    "\t\tword_overlap_sparse = scipy.sparse.coo_matrix(numpy.array(word_overlap)) \n",
    "\n",
    "\treturn word_overlap_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting tf-idf vectors (for both the bodies and the headlines).\n",
    "def extract_tfidf(training_headlines, training_bodies, dev_headlines, dev_bodies, test_headlines, test_bodies):\n",
    "\t# Body vectorisation\n",
    "\tbody_vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, stop_words='english')#, max_features=1024)\n",
    "\tbodies_tfidf = body_vectorizer.fit_transform(training_bodies)\n",
    "\n",
    "\t# Headline vectorisation\n",
    "\theadline_vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, stop_words='english')#, max_features=1024)\n",
    "\theadlines_tfidf = headline_vectorizer.fit_transform(training_headlines)\n",
    "\n",
    "\t# Tranform dev/test bodies and headlines using the trained vectorizer (trained on training data)\n",
    "\tbodies_tfidf_dev = body_vectorizer.transform(dev_bodies)\n",
    "\theadlines_tfidf_dev = headline_vectorizer.transform(dev_headlines)\n",
    "\n",
    "\tbodies_tfidf_test = body_vectorizer.transform(test_bodies)\n",
    "\theadlines_tfidf_test = headline_vectorizer.transform(test_headlines)\n",
    "\n",
    "\t# Combine body_tfdif with headline_tfidf for every data point. \n",
    "\ttraining_tfidf = scipy.sparse.hstack([bodies_tfidf, headlines_tfidf])\n",
    "\tdev_tfidf = scipy.sparse.hstack([bodies_tfidf_dev, headlines_tfidf_dev])\n",
    "\ttest_tfidf = scipy.sparse.hstack([bodies_tfidf_test, headlines_tfidf_test])\n",
    "\n",
    "\treturn training_tfidf, dev_tfidf, test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the cosine similarity between bodies and headlines. \n",
    "def extract_cosine_similarity(headlines, bodies):\n",
    "\tvectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=True, stop_words='english')#, max_features=1024)\n",
    "\n",
    "\tcos_sim_features = []\n",
    "\tfor i in range(0, len(bodies)):\n",
    "\t\tbody_vs_headline = []\n",
    "\t\tbody_vs_headline.append(bodies[i])\n",
    "\t\tbody_vs_headline.append(headlines[i])\n",
    "\t\ttfidf = vectorizer.fit_transform(body_vs_headline)\n",
    "\n",
    "\t\tcosine_similarity = (tfidf * tfidf.T).A\n",
    "\t\tcos_sim_features.append(cosine_similarity[0][1])\n",
    "\n",
    "\t# Convert the list to a sparse matrix (in order to concatenate the cos sim with other features)\n",
    "\tcos_sim_array = scipy.sparse.coo_matrix(numpy.array(cos_sim_features)) \n",
    "\n",
    "\treturn cos_sim_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting words\n",
    "def extract_word_counts(headlines, bodies):\n",
    "\tword_counts = []\n",
    "\n",
    "\tfor i in range(0, len(headlines)):\n",
    "\t\tfeatures = []\n",
    "\t\tfeatures.append(len(headlines[i].split(\" \")))\n",
    "\t\tfeatures.append(len(bodies[i].split(\" \")))\n",
    "\t\tword_counts.append(features)\n",
    "\tword_counts_array = scipy.sparse.coo_matrix(numpy.array(word_counts))\n",
    "\n",
    "\treturn word_counts_array \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for combining features of various types (lists, coo_matrix, np.array etc.)\n",
    "def combine_features(tfidf_vectors, cosine_similarity, word_overlap):\n",
    "\tcombined_features =  sparse.bmat([[tfidf_vectors, word_overlap.T, cosine_similarity.T]])\n",
    "\treturn combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting features\n",
    "# Feautres: 1) Word Overlap, 2) TF-IDF vectors, 3) Cosine similarity, 4) Word embeddings\n",
    "def extract_features(train, dev, test):\n",
    "\t# Get bodies and headlines for dev and training data\n",
    "\ttraining_bodies = get_bodies(training_data)\n",
    "\ttraining_headlines = get_headlines(training_data)\n",
    "\tdev_bodies = get_bodies(dev_data)\n",
    "\tdev_headlines = get_headlines(dev_data)\n",
    "\ttest_bodies = get_bodies(test_data)\n",
    "\ttest_headlines = get_headlines(test_data)\n",
    "\n",
    "\t# Extract tfidf vectors\n",
    "\tprint(\"\\t-Extracting tfidf vectors..\")\n",
    "\ttraining_tfidf, dev_tfidf, test_tfidf = extract_tfidf(training_headlines, training_bodies, dev_headlines, dev_bodies, test_headlines, test_bodies)\n",
    "\n",
    "\n",
    "\t# Extract word overlap \n",
    "\tprint(\"\\t-Extracting word overlap..\")\n",
    "\ttraining_overlap = extract_word_overlap(training_headlines, training_bodies)\n",
    "\tdev_overlap = extract_word_overlap(dev_headlines, dev_bodies)\n",
    "\ttest_overlap = extract_word_overlap(test_headlines, test_bodies)\n",
    "\n",
    "\t# Extract cosine similarity between bodies and headlines. \n",
    "\tprint(\"\\t-Extracting cosine similarity..\")\n",
    "\ttraining_cos = extract_cosine_similarity(training_headlines, training_bodies)\n",
    "\tdev_cos = extract_cosine_similarity(dev_headlines, dev_bodies)\n",
    "\ttest_cos = extract_cosine_similarity(test_headlines, test_bodies)\n",
    "\n",
    "\t# Combine the features\n",
    "\ttraining_features = combine_features(training_tfidf, training_cos, training_overlap)\n",
    "\tdev_features = combine_features(dev_tfidf, dev_cos, dev_overlap)\n",
    "\ttest_features = combine_features(test_tfidf, test_cos, test_overlap)\n",
    "\n",
    "\treturn training_features, dev_features, test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-Training size:\t 40106\n",
      "\t-Dev size:\t 4835\n",
      "\t-Test data:\t 5031\n"
     ]
    }
   ],
   "source": [
    "data_splits = split(dataset)\n",
    "training_data = data_splits['training']\n",
    "dev_data = data_splits['dev']\n",
    "test_data = data_splits['test']\n",
    "N = int(len(training_data) * 1.0)\n",
    "training_data = training_data[:N]\n",
    "print(\"\\t-Training size:\\t\", len(training_data))\n",
    "print(\"\\t-Dev size:\\t\", len(dev_data))\n",
    "print(\"\\t-Test data:\\t\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-Extracting tfidf vectors..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:00, 346.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-Extracting word overlap..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40106it [02:28, 270.45it/s]\n",
      "4835it [00:12, 379.65it/s]\n",
      "5031it [00:13, 380.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-Extracting cosine similarity..\n"
     ]
    }
   ],
   "source": [
    "training_features, dev_features, test_features = extract_features(training_data, dev_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=150, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C = 1.0, class_weight='balanced', solver=\"lbfgs\", max_iter=150) \n",
    "targets_tr = [a['Stance'] for a in training_data]\n",
    "targets_dev = [a['Stance'] for a in dev_data]\n",
    "targets_test = [a['Stance'] for a in test_data]\n",
    "lr.fit(training_features, targets_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    179    |    32     |    86     |    27     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    13     |    49     |    23     |     4     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    112    |    39     |    824    |    22     |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    48     |    20     |    57     |   3496    |\n",
      "-------------------------------------------------------------\n",
      "Score: 2002.25 out of 2315.25\t(86.4809415829824%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix\")\n",
    "score.report_score(targets_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
